{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is linear regression?\n",
    "\n",
    "Linear = our predictions are a **linear combination** of our inputs\n",
    "\n",
    "Regression = we will learn the relationship that relates features to labels\n",
    "\n",
    "$X$ is a matrix of training data. Each row represents a different training example (of which there are $m$). Each column represents a different feature (of which there are $n$). Hence $X$ has dimensions $m \\times n$, i.e. $X \\in  R^{m \\times n}$.\n",
    "\n",
    "$W$ is our matrix of weights, that controls how much each feature contributes to the hypothesis. If one particular weight equals 5, then changing its associated feature by 1 in the input space, will change the output hypothesis by 5. $W \\in R^{n \\times 1}$\n",
    "\n",
    "$h$ is our hypothesis - our prediction of the mapping from input to output. In this example, our model will predict a single scalar output for each of out $m$ inputs - so $h \\in R^{m\\times n}$.\n",
    "\n",
    "## $ h = X  W = w_1 x_1 + w_2 x_2 + \\dots + w_{n-1} x_{n-1} + w_n x_n$\n",
    "\n",
    "This linear combination is a **weighted sum of the input features**. As we vary the value of one feature, our hypothesis will change proportionately and linearly.\n",
    "\n",
    "Imagine that we are trying to predict house price. Consider:\n",
    "- The weight associated with the feature that is the number of rooms should be large and positive, because the number of rooms contributes lots, and positively to the price of a house. \n",
    "- The weight associated with the age of the house may be negative, as older houses might be found to be worth less from the training data.\n",
    "- The weight associated with a feature that is the age of the person last living there should be zero, because the house price is independent of this feature. It does not contribute at all to the house price.\n",
    "\n",
    "## Cost functions\n",
    "\n",
    "For our algorithms to learn, we need a way to evaluate their current performance, so that we can determine how to improve. We can mathematically define when our algorithm is performing well by evaluating an appropriate objective function. We usually try to minimise a function which indicates the error in our hypothesis. In this case, we will use the mean squared error (MSE) between our predictions and labels as our cost function.\n",
    "\n",
    "## $ MSE\\ Loss,\\ J = \\frac{1}{2m} \\sum_{i=1}^{m} (h^{(i)} - y^{(i)})^2$\n",
    "\n",
    "The cost function has as many dimensions as we have parameters. Changing these parameters moves us around parameter space, in which the cost varies. Varying different parameters will have varying influence on how the cost changes - as such, some are more important to optimise.\n",
    "\n",
    "(See cost functions notebook for more detail)\n",
    "\n",
    "## Optimization\n",
    "We optimize this model using the gradient descent algorithm where we iteratively calculate the derivative of our cost w.r.t our paramameters and use that to update our weights in a direction which reduces the cost.\n",
    "\n",
    "\n",
    "## Implementation\n",
    "Firstly we will import some functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functionality from these libraries\n",
    "%matplotlib notebook\n",
    "import numpy as np      # for efficient numerical computation\n",
    "import torch            # for building computational graphs\n",
    "from torch.autograd import Variable     # for automatically computing gradients of our cost with respect to what we want to optimise\n",
    "import matplotlib.pyplot as plt     # for plotting absolutely anything\n",
    "from mpl_toolkits.mplot3d import Axes3D # for plotting 3D graphs\n",
    "import pandas as pd #allows us to easily import any data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import our data into a pandas data frame and shuffle it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       800     0  0.3048  71.3  0.00266337  126.201\n",
      "1452  3150  12.3  0.1016  39.6    0.040827  113.055\n",
      "974   4000   0.0  0.0254  55.5    0.000412  133.223\n",
      "1306   800   3.3  0.1016  55.5    0.002211  129.119\n",
      "1421  4000  12.3  0.1016  71.3    0.033779  118.018\n",
      "1229   630  22.2  0.0254  39.6    0.022903  137.026\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('airfoil_self_noise.dat', sep='\\t')#import our dataset into a pandas dataframe\n",
    "df = df.sample(frac=1) #shuffle our dataset\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the datapoints into torch tensors, normalize our features and split intro training and test sets. It is very important to normalize in this case as our features have are different orders of magnitude. Try training without and you will see that the loss is significantly higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert our data into torch tensors\n",
    "X = torch.Tensor(np.array(df[df.columns[0:-1]])) #pick our features from our dataset\n",
    "Y = torch.Tensor(np.array(df[df.columns[-1]])) #select our label\n",
    "\n",
    "X = (X-X.mean(0)) / X.std(0) #normalize our features along the 0th axis\n",
    "\n",
    "m = 1100 #size of training set\n",
    "\n",
    "#split our data into training and test set\n",
    "#training set\n",
    "x_train = Variable(X[0:m])\n",
    "y_train = Variable(Y[0:m])\n",
    "\n",
    "#test set\n",
    "x_test = Variable(X[m:])\n",
    "y_test = Variable(Y[m:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model class which we will use to instantiate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model class - inherit useful functions and attributes from torch.nn.Module\n",
    "class linearmodel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() #call parent class initializer\n",
    "        self.linear = torch.nn.Linear(5, 1) #define linear combination function with 11 inputs and 1 output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x) #linearly combine our inputs to give 1 outputs\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the necessary hyper-parameters, instantiate the model from the class, cost function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_epochs = 100\n",
    "lr = 10\n",
    "\n",
    "#create our model from defined class\n",
    "mymodel = linearmodel()\n",
    "criterion = torch.nn.MSELoss() #cross entropy cost function as it is a classification problem\n",
    "optimizer = torch.optim.Adam(mymodel.parameters(), lr = lr) #define our optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the axes which we will use to plot our costs each epoch. Define the training loop and train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for plotting costs\n",
    "costs=[]\n",
    "plt.ion()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Cost')\n",
    "ax.set_xlim(0, no_epochs-1)\n",
    "plt.show()\n",
    "\n",
    "#training loop - same as last time\n",
    "def train(no_epochs):\n",
    "    for epoch in range(no_epochs):\n",
    "        h = mymodel.forward(x_train) #forward propagate - calulate our hypothesis\n",
    "        #calculate, plot and print cost\n",
    "        cost = criterion(h, y_train)\n",
    "        costs.append(cost.data[0])\n",
    "        ax.plot(costs, 'b')\n",
    "        fig.canvas.draw()\n",
    "        print('Epoch ', epoch, ' Cost: ', cost.data[0])\n",
    "\n",
    "        #calculate gradients + update weights using gradient descent step with our optimizer\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "train(no_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check the cost of the model on the test set. If there is a significant difference increase of the cost on the test set compared to the training set, it means that our model does not generalize well to new examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    h = mymodel.forward(x_test)\n",
    "    cost = criterion(h, y_test)\n",
    "    \n",
    "    return cost.data[0]\n",
    "\n",
    "test_cost = test()\n",
    "print('Cost on test set: ', test_cost)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
