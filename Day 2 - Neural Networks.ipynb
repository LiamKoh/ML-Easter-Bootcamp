{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "A Neural Network in a powerful data modelling tool which allows us to represent and learn arbitrarily complex functions. that has got so many people excited about Machine Learning as of late. It takes inspiration from the hierarchical structure of the visual processing systems found in many animals including humans.<br>\n",
    "\n",
    "A fully connected feed-forward neural network consists of layers of nodes, each node in each layer connected to each node in the previous layer with a weighted edge. The first layer is called the input layer, the last layer is called the output layer and all layers in between are called hidden layers. There are many other different ways that neural networks can be connected (architectures); in this bootcamp we will concern ourselves with deep neural networks (DNNs), convolutional neural networks (CNNs) & recurrent neural networks (RNNs).\n",
    "\n",
    "![](nn.png)\n",
    "\n",
    "Each node performs a very simple calculation. On a forward pass, where you feed values into the input layer, each node calculates a weighted sum/linear combination of the nodes in the previous layer and applies an activation function. This is done sequentially through all the layers to get your output. The weights that each node uses to calculate its output are the parameters of the model and they need to be optimised in order to get the required output. Note that layer nodes do not perform any calculation.\n",
    "\n",
    "There are many different activation functions which have been shown to work including sigmoid, tanh, RELU, ELU and more are being researched everyday. RELU is the most commonly used one. But for classification problems, a sigmoid activation is used on the output layer to limit the output between 0 and 1 to represent a probability.\n",
    "\n",
    "![](activation.png)\n",
    "\n",
    "To train these networks, we use good old gradient descent. We feed in the input for each data point in our training set to calculate the output. We compare this output to the labels to calculate the loss and then calculate the derivative of our loss w.r.t each weight in the network. We update the weights using this gradient and perform the whole process iteratively until we are satisfied with how low the loss is. Gradient descent applied to optimizing a neural network is commonly known as back-propagation.\n",
    "\n",
    "What tends to happen with these networks one they are trained, especially when deeper architectures are used is that the first layers find low level features from the inputs. In the case of image processing, the neurons in the first layers activate for different types of lines and curves in the image while the the middle layers activate for different combinations of the lower level features such as basic shapes. The final layers activate for even higher level features such as (if we're tring to classify faces) ears, eyes and noses. And of course the neurons in the final layer activate for the highest level features we have which are different faces. This is exactly the kind of representations which are built up in biological visual processing systems.\n",
    "\n",
    "![](nnfeaturevis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "We are going to be building a neural network which takes in as input 30 features which are different measurements taken from a tumor and outputs the probability of it being malignant. If a tumor is malignant it means that it is cancerous so you can see how useful this kind of model could be in the medical profession to assist with decision making.\n",
    "\n",
    "First, we import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset into a Pandas dataframe, map our class labels into numerical values and shuffle our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cancer_data.csv')\n",
    "df[['diagnosis']] = df['diagnosis'].map({'M': 1, 'B': 0}) #map into numeric data\n",
    "df = df.sample(frac=1) #shuffle dataset\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the required data from the dataframe into torch tensors so we can use them in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.Tensor(np.array(df[df.columns[2:-1]])) #pick our features from our dataset\n",
    "Y = torch.Tensor(np.array(df[['diagnosis']])) #select out label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose how many datapoints we will use as our training set and split intro train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 450 #size of training set\n",
    "\n",
    "x_train = Variable(X[:m])\n",
    "y_train = Variable(Y[:m])\n",
    "\n",
    "x_test = Variable(X[m:])\n",
    "y_test = Variable(Y[m:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our Neural Network model using PyTorch's class interface. We define the Net class and inherit from torch.nn.module.<br>\n",
    "We define the required linear combination operations in the initializer function.<br>\n",
    "We define the forward-propagation, applying the linear combination operations defined in the initializer function followed by an activation function at each layer.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() #call parent class initializer\n",
    "        self.h1 = torch.nn.Linear(30, 10) #input layer to size 10 hidden layer\n",
    "        self.out = torch.nn.Linear(10, 1) #hidden layer to single output\n",
    "\n",
    "    #define the forward propagation/prediction equation of our model\n",
    "    def forward(self, x):\n",
    "        x = self.h1(x) #linear combination\n",
    "        x = F.relu(x) #activation\n",
    "        x = self.out(x) #linear combination\n",
    "        x = F.sigmoid(x) #activation\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the training hyperparameters, cost function and optimizer. Instantiate a model from the class we defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training hyper-parameters\n",
    "no_epochs = 150 #number of epochs to train for\n",
    "lr = 0.0005 #learning rate\n",
    "\n",
    "mynet = Net() #instantiate model from class\n",
    "criterion = torch.nn.BCELoss() #define cost function\n",
    "optimizer = torch.optim.Rprop(mynet.parameters(), lr=lr) #choose optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the axes which we will use to plot our loss function as we train. <br>\n",
    "Define the function which we will use to train our model and train for the number of epochs specified by the no_epochs variable earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = []\n",
    "plt.ion()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Cost')\n",
    "ax.set_xlim(0, no_epochs)\n",
    "\n",
    "def train(no_epochs, lr):\n",
    "    for epoch in range(no_epochs):\n",
    "        #forward propagate - calulate our hypothesis\n",
    "        h = mynet.forward(x_train)\n",
    "\n",
    "        #calculate, plot and print cost\n",
    "        cost = criterion(h, y_train)\n",
    "        costs.append(cost.data[0])\n",
    "        ax.plot(costs, 'b')\n",
    "        fig.canvas.draw()\n",
    "        print('Epoch ', epoch+1, ' Cost: ', cost.data[0])\n",
    "\n",
    "        #backpropagate + gradient descent step\n",
    "        optimizer.zero_grad() #set gradients to zero\n",
    "        cost.backward() #backpropagate to calculate derivatives\n",
    "        optimizer.step() #update our weights\n",
    "\n",
    "train(no_epochs, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calulate the accuracy of our model on our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_h = mynet.forward(x_test) #predict values for the test set\n",
    "test_h.data.round_() #round output probabilities to give us discrete predictions\n",
    "correct = test_h.data.eq(y_test.data) #perform element-wise equality operation\n",
    "accuracy = torch.sum(correct)/correct.shape[0] #calculate accuracy\n",
    "print('Test accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the optimized model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mynet.state_dict(), 'mynet_trained')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
