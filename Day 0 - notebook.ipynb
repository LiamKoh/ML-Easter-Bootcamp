{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression in numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook gives an intro to machine learning from scratch using numpy and also covers feature scaling and the gradient descent algorithm.\n",
    "\n",
    "## What is linear regression?\n",
    "\n",
    "Linear = our predictions are a **linear combination** of our inputs\n",
    "\n",
    "Regression = we will learn the relationship that relates features to labels\n",
    "\n",
    "$h$ is our hypothesis - our prediction of the mapping from input to output.\n",
    "\n",
    "## $ h = W X = w_1 x_1 + w_2 x_2 + \\dots + w_{n-1} x_{n-1} + w_n x_n$\n",
    "\n",
    "This linear combination is a **weighted sum of the input features**. As we vary the value of one feature, our hypothesis will change proportionately and linearly.\n",
    "\n",
    "Imagine that we are trying to predict house price. Consider:\n",
    "- The weight associated with the feature that is the number of rooms will be large and positive, because the number of rooms contributes lots, and positively to the price of a house. \n",
    "- The weight associated with the age of the house may be negative, as older houses might be found to be worth less from the training data.\n",
    "- The weight associated with a feature that is the age of the person last living there will be zero, because the house price is independent of this feature. It does not contribute at all to the house price.\n",
    "\n",
    "## Gradient Descent\n",
    "Gradient descent works by moving the weights that control a model\n",
    "\n",
    "\n",
    "Firstly we will import some functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Data\n",
    "Let's write a function and call it to create some fake data that we know the function of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100 # specify the number of datapoints that we want\n",
    "\n",
    "def makedata(numdatapoints):            # make some fake data fitted to an arbitrary polynomial\n",
    "    \"\"\"Make some fake data noisily distributed around a polynomial\"\"\"\n",
    "    x = np.linspace(-10, 10, numdatapoints)     # create a vector of numdatapoints(=m) numbers evenly spaced between -10 and 10\n",
    "    x = x.reshape(-1, 1)    # make it into a column vector (each datapoint is a row)\n",
    "\n",
    "    coeffs = [2, -30, 0.5, 5]   # make some polynomial coefficients for the fake data\n",
    "\n",
    "    y = np.polyval(coeffs, x) + 2 * np.random.rand(numdatapoints, 1)    # evaluate a polynomial with the coefficients we\n",
    "                                                                        # specified to create the labels for our data\n",
    "    y = y.reshape(-1, 1) # reshape into column vector (each datapoint is a row)\n",
    "\n",
    "    return x, y # return column vectors of single inputs and outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "\n",
    "We may choose to give the model a particular set of features based upon how we expect the mapping to look. For example, house price is probably dependent on floor space, so we might include a power of 2 if our original variable was room length.\n",
    "\n",
    "Let's manually **make some polynomial features** from our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "powers = [2, 3]     # a list of the powers of the inputs which we want to include as features for our model\n",
    "n = len(powers)     # n = number of features of each training datapoint\n",
    "\n",
    "def makefeatures(powers):\n",
    "    features = np.ones((inputs.shape[0], len(powers)))  # initialise a design matrix with the right shape (mxn)\n",
    "    for i in range(len(powers)):    # for each power in the list powers\n",
    "        features[:, i] = (inputs**powers[i])[:, 0] # set a column of the design matrix = inputs raised to that power\n",
    "    print(features)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling - normalisation/standardisation\n",
    "\n",
    "If the scale of variables is different, it is likely that one of the variables will start further from an acceptably sufficient position than another. When the first one of them reaches a sufficient value, it needs to stop jumping around and settle, however, any other variables need to keep moving and being optimised. \n",
    "If variables vary over the same domain, but have ranges of different scales\n",
    "The learning rate needs to be small enough to ensure that the parameters converge, but large enough to ensure that this happens at a suitable rate.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build our model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel():\n",
    "    \n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.weights = np.random.rand((outputs, inputs + 1)) # plus one row for the bias\n",
    "        \n",
    "    def forward(x):\n",
    "        h = np.matmul(self.weights, x)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function to train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    costs = [] # initialise an empty list to store our past costs in\n",
    "    for e in range(epochs): # for as many epochs as we have defined\n",
    "        prediction = mymodel(datain) # pass the datain forward through our model\n",
    "        \n",
    "        cost = criterion(predictions, labels) # calculate the cost of our predictions compared to the labels\n",
    "        costs.append(cost.data) # get the data from the cost variable and append it to the list of costs\n",
    "        print('Epoch', e, 'Cost', cost.data[0])\n",
    "        \n",
    "        # unpack the parameters so that we can use values for visualisation\n",
    "        params = [mymodel.state_dict([i][0]) for i in mymodel.state_dict()]\n",
    "        weights = params[0]\n",
    "        bias = params[1]\n",
    "        print('b', bias)\n",
    "        print('w', weights)\n",
    "        \n",
    "        optimiser.zero_grad() # zero the gradients\n",
    "        cost.backward() # push the error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's call all of our functions, train the model and then show the history of costs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
