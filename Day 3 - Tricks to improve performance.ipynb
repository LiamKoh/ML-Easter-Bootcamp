{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- vanishing and exploding gradients\n",
    "-- batch norm\n",
    "-- xavier initialization\n",
    "-- Relu activation \n",
    "\n",
    "- regularisation\n",
    "-- dropout\n",
    "-- standard\n",
    "\n",
    "- normalisation\n",
    "\n",
    "- loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two key problems that arise when training neural networks are vanishing and exploding gradients. These problems are when we have deep networks and are due to the fact that the first layers' gradients are calculated using the chain rule which requires us to multiply with the gradients of the final layer neurons. Consider the case when we are using a sigmoid activation function with the network shown below. As you can see each time we calculate the gradients for lower closer to the input, we multiply by a $\\sigma'(z_i)$ term. Given that the max value that this derivative of the sigmoid can take is 0.25 as shown in the graph below, our gradients will reduce by atleast a factor of 4 each time we add a layer. The gradient vanishes which makes learning very slow.\n",
    "\n",
    "$J = \\frac{1}{2}(h-y)^2$\n",
    "\n",
    "$h = \\sigma(z_3)$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$z_3 = w_3a_2$\n",
    "\n",
    "$a_2 = \\sigma(z_2)$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$z_2 = w_2a_1$\n",
    "\n",
    "$a_1 = \\sigma(z_1)$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$z_1 = w_1x$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial w_3} = (h-y)\\ \\sigma'(z_3)\\ a_2$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial w_2} = (h-y)\\ \\sigma'(z_3)\\ w_3\\ \\sigma'(z_2)\\ a_1$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial w_1} = (h-y)\\ \\sigma'(z_3)\\ w_3\\ \\sigma'(z_2)\\ w_2\\ \\sigma'(z_1)\\ x$\n",
    "\n",
    "![](sigderiv.png)\n",
    "\n",
    "In certain cases, the w terms can get very high and after being multiplied all the way through the network can lead to an extremely high gradient causing us to jump about all over the cost surface, never converging. This is called exploding gradients and occurs less frequently than vanishing gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
