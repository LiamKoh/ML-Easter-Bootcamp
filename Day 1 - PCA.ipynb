{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "One challenge in machine learning is finding meaning in the data, particularly when it is very high dimensional and can be difficult to visualise. As such **dimensionality reduction**, which is a set of techniques that reduce the number of features in data, can be valuable. Reducing the number of features can make data visualisable, so that the problem can become easier to understand, and greatly reduce the number of model parameters which need to be optimised, which can save time and data.\n",
    "\n",
    "One dimensionality reduction technique which we will look at in this bootcamp is pricipal component analysis (PCA). It involves linearly transforming the normalised data such that it varies mostly in the directions of its axes, that are then known as the **pricipal components** of the data. Doing so means that the most important information (that of greatest variation) is captured as the 1st principal component of that data, the second most along the 2nd principal component, the 3rd most along the 3rd and so on. As such, the less important information, which lies along higher principal components, can be discarded by ignoring the values of the data which lie in that direction. Mathematically, this is a projection onto the lower dimensional space spanned by the more important principal components.\n",
    "\n",
    "So how do you capture the most important information? If we had a 3-d input space, but all of the data points lied at the same height above the x-y plane, then it would be obvious that the z axis values of each of the datapoints could be discarded. This is not because the z-values take any particular value, but because they have zero **variance**. Variance is the mean of the square differences of the feature from that features' mean value:\n",
    "\n",
    "## $Var(x) = \\sigma^2 = \\frac{\\sum_i^m (x_i - \\mu)^2}{m}$\n",
    "\n",
    "In more than one dimensional space, data variance is characterised by a **covariance matrix**. Whereas the variance characterises averagely how much the feature varies from its mean, the covariance matrix defines how much the \n",
    "\n",
    "The first step is to normalise the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
