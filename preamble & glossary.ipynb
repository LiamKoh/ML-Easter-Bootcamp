{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootcamp Preamble\n",
    "\n",
    "## Datapoint\n",
    "\n",
    "Here a single data point, $x^{(1)}$ is represented as a vector where each row is a different **feature**. \n",
    "\n",
    "For example, if each training example is a house, then its vector of features may include elements for its price, no. rooms, no. windows etc.\n",
    "\n",
    "### $X^{(1)} =  \\begin{bmatrix} x_1\\\\ x_2\\\\ \\vdots \\\\ x_{n-1}\\\\ x_n \\end{bmatrix}$\n",
    "\n",
    "## Design Matrix\n",
    "The **design matrix**, **X** contains all of our training data. Each column represents a certain example. There are $m$ training examples. Each row represents a different feature. There are $n$ features. Hence the design matrix has dimensions of $n$ by $m$.\n",
    "\n",
    "### $Design \\ matrix,\\ X = \\begin{bmatrix} \\vdots & & \\vdots \\\\ x^{(1)} & \\dots &  x^{(m)} \\\\ \\vdots & & \\vdots \\end{bmatrix} = \\begin{bmatrix} x_{11} \\dots x_{1m} \\\\ \\vdots \\ddots \\vdots \\\\ x_{n1} \\dots x_{nm} \\end{bmatrix} \\in n \\times m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n",
    "The hypothesis, $h$ is the output of your model. It is your current prediction of the mapping from input to output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss/cost function\n",
    "\n",
    "Your loss function is a function which you use to measure how bad your model is. We will represent the loss of our models with the symbol $J$.\n",
    "\n",
    "#### Mean Squared Error Loss\n",
    "MSE loss is the average over all training points of the squared error between your hypothesis and the label. The factor of $\\frac{1}{2}$ is often included to cancel with the power of 2 when differentiated so that no constants are present.\n",
    "\n",
    "### $ J = \\sum_{i=1}^{m} \\frac{1}{2m}(h - y)^2$\n",
    "\n",
    "#### Binary Cross entropy loss\n",
    "BCE loss is used to calculate error for classification tasks. \n",
    "\n",
    "### $ J = \\sum_{i=1}^{m} - y \\cdot \\text{log}(h) + (1-y) \\cdot \\text{log}(1-h)$\n",
    "\n",
    "In classification tasks, for each class the label of a datapoint can only take binary values of 0 or 1; i.e. it *is* a member of that class or it *is not* a member of that class, and the output is usually a *confidence* value $\\in [0, 1]$.\n",
    "When , $y, = 0$ the first term is 'turned off' and the second term "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kullback-Leibler Divergence\n",
    "The KL divergence is a metric that quantifies the difference between two probability distributions, $p$ & $q$. It is used frequently in machine learning to measure the information lost when we try to represent a probability distribution in a different way (e.g. after reconstructing it from an encoding).\n",
    "\n",
    "### $D_{KL}(p||q) = \\sum_{i=1}^{m} p(x_i)\\cdot (\\text{log }p(x_i) - \\text{log }q(x_i)) = \\sum_{i=1}^{m} p(x_i)\\cdot \\text{log } \\frac{p(x_i)}{q(x_i)}$\n",
    "\n",
    "For a single datapoint, $x$, the KL divergence tests how similar the log probabilities of that value are and weights that difference by the value of the probability of sampling that $x$ from $p(x)$. The weighting $p(x)$ of the log difference makes the KL divergence different depending on which arrangement you compare the probability distributions in.\n",
    "\n",
    "Consider:\n",
    "- It takes large values when the sampled probabilities for the same values are more different, and the weighting probability distribution $p(x)$ is larger. \n",
    "- It takes a value of zero where the weighting probability distribution is zero.\n",
    "- Where the \n",
    "- The aim is often to minimise the KL divergence (the information difference between two probability distributions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
